{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Stable Diffusion API Server for Multimodal RAG\n",
                "\n",
                "This notebook sets up a Stable Diffusion API server on Google Colab GPU that can be accessed from your local machine.\n",
                "\n",
                "**Requirements:**\n",
                "- Google Colab with GPU runtime (T4 or better)\n",
                "- ngrok account (free) for tunneling\n",
                "- (Optional) Hugging Face token for gated models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q diffusers transformers accelerate safetensors flask pyngrok pillow"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. (Optional) Setup Hugging Face Token\n",
                "\n",
                "**Only needed if using gated models like stable-diffusion-2-1**\n",
                "\n",
                "Get your token from: https://huggingface.co/settings/tokens"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option 1: Use Colab Secrets (Recommended)\n",
                "# Add HF_TOKEN to your Colab secrets\n",
                "from google.colab import userdata\n",
                "try:\n",
                "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
                "    print(\"‚úÖ HF_TOKEN loaded from Colab secrets\")\n",
                "except:\n",
                "    print(\"‚ö†Ô∏è HF_TOKEN not found in secrets\")\n",
                "    # Option 2: Paste token directly (less secure)\n",
                "    HF_TOKEN = \"\"  # Paste your token here if needed\n",
                "\n",
                "# Login to Hugging Face\n",
                "if HF_TOKEN:\n",
                "    from huggingface_hub import login\n",
                "    login(token=HF_TOKEN)\n",
                "    print(\"‚úÖ Logged in to Hugging Face\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No HF token provided. Will use public models only.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Stable Diffusion Model\n",
                "\n",
                "**Choose one of the following models:**\n",
                "- `runwayml/stable-diffusion-v1-5` - No auth required, fast\n",
                "- `stabilityai/stable-diffusion-2-1` - Requires HF token, better quality\n",
                "- `CompVis/stable-diffusion-v1-4` - No auth required"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
                "from PIL import Image\n",
                "import io\n",
                "import base64\n",
                "\n",
                "# Model configuration - Choose one:\n",
                "# Option 1: No authentication required (Recommended for quick start)\n",
                "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
                "\n",
                "# Option 2: Better quality but requires HF token\n",
                "# model_id = \"stabilityai/stable-diffusion-2-1\"\n",
                "\n",
                "# Option 3: Alternative public model\n",
                "# model_id = \"CompVis/stable-diffusion-v1-4\"\n",
                "\n",
                "print(f\"Loading Stable Diffusion model: {model_id}...\")\n",
                "\n",
                "try:\n",
                "    pipe = StableDiffusionPipeline.from_pretrained(\n",
                "        model_id,\n",
                "        torch_dtype=torch.float16,\n",
                "        safety_checker=None,\n",
                "        use_auth_token=HF_TOKEN if 'HF_TOKEN' in globals() and HF_TOKEN else None\n",
                "    )\n",
                "    \n",
                "    # Use DPM solver for faster generation\n",
                "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
                "    \n",
                "    # Move to GPU\n",
                "    pipe = pipe.to(\"cuda\")\n",
                "    \n",
                "    print(\"‚úÖ Model loaded successfully!\")\n",
                "    print(f\"   Model: {model_id}\")\n",
                "    print(f\"   Device: {pipe.device}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error loading model: {e}\")\n",
                "    print(\"\\nTroubleshooting:\")\n",
                "    print(\"1. If using stabilityai/stable-diffusion-2-1:\")\n",
                "    print(\"   - Accept the license at: https://huggingface.co/stabilityai/stable-diffusion-2-1\")\n",
                "    print(\"   - Add HF_TOKEN to Colab secrets\")\n",
                "    print(\"2. Or use runwayml/stable-diffusion-v1-5 (no auth needed)\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Create Flask API Server"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from flask import Flask, request, jsonify, send_file\n",
                "from io import BytesIO\n",
                "\n",
                "app = Flask(__name__)\n",
                "\n",
                "@app.route('/health', methods=['GET'])\n",
                "def health():\n",
                "    return jsonify({\"status\": \"healthy\", \"model\": model_id})\n",
                "\n",
                "@app.route('/generate', methods=['POST'])\n",
                "def generate():\n",
                "    try:\n",
                "        data = request.json\n",
                "        \n",
                "        prompt = data.get('prompt', '')\n",
                "        negative_prompt = data.get('negative_prompt', 'blurry, bad quality, distorted')\n",
                "        num_inference_steps = data.get('num_inference_steps', 30)\n",
                "        guidance_scale = data.get('guidance_scale', 7.5)\n",
                "        height = data.get('height', 512)\n",
                "        width = data.get('width', 512)\n",
                "        \n",
                "        print(f\"Generating image for prompt: {prompt}\")\n",
                "        \n",
                "        # Generate image\n",
                "        image = pipe(\n",
                "            prompt=prompt,\n",
                "            negative_prompt=negative_prompt,\n",
                "            num_inference_steps=num_inference_steps,\n",
                "            guidance_scale=guidance_scale,\n",
                "            height=height,\n",
                "            width=width\n",
                "        ).images[0]\n",
                "        \n",
                "        # Convert to bytes\n",
                "        img_io = BytesIO()\n",
                "        image.save(img_io, 'PNG')\n",
                "        img_io.seek(0)\n",
                "        \n",
                "        return send_file(img_io, mimetype='image/png')\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error: {e}\")\n",
                "        return jsonify({\"error\": str(e)}), 500\n",
                "\n",
                "print(\"‚úÖ Flask app created!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Setup ngrok Tunnel\n",
                "\n",
                "Get your ngrok auth token from: https://dashboard.ngrok.com/get-started/your-authtoken"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyngrok import ngrok\n",
                "\n",
                "# Set your ngrok auth token\n",
                "NGROK_AUTH_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"  # Replace with your token\n",
                "\n",
                "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
                "\n",
                "# Create tunnel\n",
                "public_url = ngrok.connect(5000)\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üöÄ Stable Diffusion API is running!\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nPublic URL: {public_url}\")\n",
                "print(\"\\nAdd this URL to your .env file:\")\n",
                "print(f\"SD_API_URL={public_url}\")\n",
                "print(\"\\n\" + \"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Start the Server\n",
                "\n",
                "**Important:** Keep this cell running to maintain the API server."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run the Flask app\n",
                "app.run(port=5000)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test the API (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "from PIL import Image\n",
                "from io import BytesIO\n",
                "\n",
                "# Test generation\n",
                "url = str(public_url) + \"/generate\"\n",
                "\n",
                "payload = {\n",
                "    \"prompt\": \"a beautiful sunset over the ocean, vibrant colors, photorealistic\",\n",
                "    \"num_inference_steps\": 30,\n",
                "    \"guidance_scale\": 7.5\n",
                "}\n",
                "\n",
                "print(\"Sending request...\")\n",
                "response = requests.post(url, json=payload, timeout=120)\n",
                "\n",
                "if response.status_code == 200:\n",
                "    img = Image.open(BytesIO(response.content))\n",
                "    display(img)\n",
                "    print(\"‚úÖ Image generated successfully!\")\n",
                "else:\n",
                "    print(f\"‚ùå Error: {response.status_code}\")\n",
                "    print(response.text)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}